{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Probabilistic Anomaly Detection by Hand\n",
    "* Given the following data points, fit a Gaussian distribution to it by estimating $\\mu$ and $\\sigma$:\n",
    "\n",
    "  $[10,  7, 11,  9, 10, 11]$\n",
    "\n",
    "* Calculate the likelihood of the points \n",
    "\n",
    "  $[2, 10, 20, 40]$\n",
    "\n",
    "* Calculate the outlier score (Negative Log Likelihood, NLL) of each point\n",
    "\n",
    "* Calculate the z-value score of each point (based on the estimated $\\mu$ and $\\sigma$)\n",
    "\n",
    "* Calculate the t-value score of each point (based on the estimated $\\mu$ and $\\sigma$)\n",
    "\n",
    "\n",
    "1. **Answer:**\n",
    "\n",
    "`please do the calculations by hand to exercise`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Probabilistic Anomaly Detection in Python\n",
    "* Implement the calculation in python. Calculate the scores for the additional points $[30, 50]$. Plot all three scores for each of the 6 points $[2, 10, 20, 40, 30, 50]$. What connection between each pair of scores do you observe? Hint: You can try to apply the mathematical transformation on AD0102 Introduction and Overview Slide 62.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code:**\n",
    "\n",
    "`<add your code below>`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "`<add your answer here>`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Gaussian Mixture Model and the EM Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian distributions\n",
    "The Gaussian distribution is a continuous probability distribution that is characterized by its mean and variance. The probability density function (pdf) of the Gaussian distribution is given by:\n",
    "\n",
    "$$\n",
    "f(x|\\mu, \\sigma^2) = \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\left(-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2\\right)\n",
    "$$\n",
    "\n",
    "where $\\mu$ is the mean and $\\sigma^2$ is the variance of the distribution.\n",
    "\n",
    "### Multivariate Gaussian distribution\n",
    "We apply the EM algorithm to fit multivariate Gaussian distributions. The multivariate Gaussian distribution is a generalization of the one-dimensional (univariate) Gaussian distribution to higher dimensions. The probability density function (pdf) of the multivariate Gaussian distribution is given by:\n",
    "\n",
    "$$\n",
    "f(\\mathbf{x}|\\mathbf{\\mu}, \\Sigma) = \\frac{1}{(2\\pi)^{d/2}\\det(\\Sigma)^{1/2}} \\exp\\left(-\\frac{1}{2}(\\mathbf{x}-\\mathbf{\\mu})^\\top\\Sigma^{-1}(\\mathbf{x}-\\mathbf{\\mu})\\right)\n",
    "$$\n",
    "\n",
    "where $\\mathbf{\\mu}$ is the mean vector, $\\Sigma$ is the covariance matrix, and $d$ is the number of dimensions of the distribution. Note, $\\Sigma^{-1}$ denotes the [inverse](https://en.wikipedia.org/wiki/Invertible_matrix) of the covariance matrix $\\Sigma$, and $\\det$ denotes the [determinant](https://en.wikipedia.org/wiki/Determinant)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Implement the function calculating this probability density"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`please write your code above in the ''#### YOUR CODE HERE' sections`\n",
    "\n",
    "Hits:\n",
    "\n",
    "* Useful functions: `numpy.sqrt, numpy.linalg.det, numpy.linalg.inv, numpy.exp, numpy.dot`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def multivariate_gaussian(x, mu, sigma):\n",
    "    #### YOUR CODE HERE\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EM Algorithm\n",
    "\n",
    "The Expectation-Maximization (EM) algorithm is an iterative method to find maximum likelihood estimates of parameters in statistical models, where the model depends on unobserved latent variables. The EM algorithm consists of two steps: the E-step and the M-step.\n",
    "\n",
    "#### E-step (Expectation step): \n",
    "In this step, the expected probabilies for the latent variables are computed given the observed data and the current estimate of the parameters. During this step the parameters $\\mu$, $\\Sigma$, and $\\pi_k$ are kept fixed.\n",
    "\n",
    "In this step we need to calculate:\n",
    "\n",
    "$$\n",
    "P(x|G_k) = g(x;\\Sigma_k, \\mu_k)\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(x) = \\sum_{k=1}^{K} \\pi_k \\cdot P(x|G_k)\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(G_k|x) = \\pi_k \\frac{P(x|G_k)}{P(x)}\n",
    "$$\n",
    "\n",
    "where $P(x|G_k)$ is the probability of the data point $x$ given the Gaussian $G_k$, \n",
    "$P(x)$ is the likelihood of the point in the current model and $P(G_k|x)$ is the probability of the point $x$ belonging to the Gaussian $G_k$.\n",
    "\n",
    "#### M-Step (Maximization step): \n",
    "In this step, the parameters $\\mu$, $\\Sigma$, and $\\pi_k$ are updated to maximize the likelihood function\n",
    "$$\n",
    "E(M) = \\sum_{x\\in D} log(P(x))\n",
    "$$\n",
    "\n",
    "We update the parameters according to the following formulas:\n",
    "\n",
    "$$\n",
    "\\pi_k = \\frac{1}{n}\\sum_{x\\in D} P(G_k|x)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mu_k = \\frac{\\sum_{x\\in D} x\\cdot P(G_k|x)}{\\sum_{x\\in D}P(G_k|x)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\Sigma_k = \\frac{\\sum_{x\\in D} P(G_k|x)(x-\\mu_k)(x-\\mu_k)^\\top}{\\sum_{x\\in D}P(G_k|x)}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementing the EM-Algorithm\n",
    "\n",
    "Implement\n",
    "- the E step\n",
    "- the M step\n",
    "- the main loop where the E and M steps are called alternately until either a maximum number of iterations is reached\n",
    "in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`please write your code above in the ''#### YOUR CODE HERE' sections`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def e_step(D: np.array, mu: np.array, sigma: np.array, pi: np.array):\n",
    "    \"\"\"\n",
    "    In this function we are given: \n",
    "    - a dataset D as a numpy array of shape (N,d) where N is the number of samples and d is the number of dimensions\n",
    "    - an numpy array of means mu of shape (k,d), where each mean is a numpy array of shape (d,) and k is the number of Gaussians\n",
    "    - a numpy array of covariance matrices sigma of shape (k,d,d), where each covariance matrix is a numpy array of shape (d,d)\n",
    "    - a list of weights pi of length k, where each weight is a float\n",
    "\n",
    "    The function should calculate and return:\n",
    "    - P_x_G: P(x|G_k) as an numpy array of shape (N,k) where each entry corresponds to the value of P(x|G_k) for each sample x and each Gaussian G_k.\n",
    "    - P_x: P(x) as an numpy array of shape (N,) where each entry corresponds to the value of P(x) for each sample x.\n",
    "    - P_G_x: P(G_k|x) as an numpy array of shape (N,k) where each entry corresponds to the value of P(G_k|x) for each sample x and each Gaussian G_k.\n",
    "    \"\"\"\n",
    "    #### YOUR CODE HERE\n",
    "    pass\n",
    "    # return P_x_G, P_x, P_G_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def m_step(D: np.array, P_G_x: np.array):\n",
    "    \"\"\"\n",
    "    In this function we are given: \n",
    "    - D: a dataset D as a numpy array of shape (N,d) where N is the number of samples and d is the number of dimensions\n",
    "    - P_G_x: P(G_k|x) as an numpy array of shape (N,k) where each entry corresponds to the value of P(G_k|x) for each sample x and each Gaussian G_k.\n",
    "\n",
    "    The function should calculate and return:\n",
    "    - mu as an numpy array of shape (k,d), where each mean is a numpy array of shape (d,)\n",
    "    - sigma as an numpy array of shape (k,d,d), where each covariance matrix is a numpy array of shape (d,d)\n",
    "    - pi as a list of weights of length k, where each weight is a float\n",
    "    \"\"\"\n",
    "    #### YOUR CODE HERE\n",
    "    pass\n",
    "    # return mu, sigma, pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def em_algorithm(D, num_gaussians, num_iterations, initial_mu = None, initial_sigma = None, initial_pi=None):\n",
    "    \"\"\"\n",
    "    In this function we are given: \n",
    "    - D: a dataset D as a numpy array of shape (N,d) where N is the number of samples and d is the number of dimensions\n",
    "    - num_gaussians: an integer specifying the number of gaussians\n",
    "    - num_iterations: an integer specifying the number of iterations\n",
    "    - optionally:\n",
    "        - initial_mu: an numpy array of shape (k,d), where each mean is a numpy array of shape (d,) and k is the number of Gaussians\n",
    "        - initial_sigma: an numpy array of shape (k,d,d), where each covariance matrix is a numpy array of shape (d,d)\n",
    "        - initial_pi: a list of weights of length k, where each weight is a float\n",
    "    - if the optional parameters are not provided, the function should initialize them \n",
    "      (mu randomly drawn from uniform distribution in (0, 1], sigma with the identity matrix, pi to uniform probabilities for each Gaussian k)\n",
    "\n",
    "    The function should calculate and return:\n",
    "    - mu as an numpy array of shape (k,d), where each mean is a numpy array of shape (d,) and k is the number of Gaussians\n",
    "    - sigma as an numpy array of shape (k,d,d), where each covariance matrix is a numpy array of shape (d,d)\n",
    "    - pi as a list of weights of length k, where each weight is a float\n",
    "    \n",
    "    The function should use the e_step and m_step functions you implemented above\n",
    "    \"\"\"\n",
    "    #### YOUR CODE HERE\n",
    "    pass\n",
    "    # return mu, sigma, pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The following code snippets are given to score the dataset after the EM fit of the GMM and to test it interactively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_samples(D, mu, sigma, pi):\n",
    "    \"\"\"\n",
    "    Calculates the log-likelihood of each sample in D under the Gaussian Mixture Model defined by mu, sigma, and pi.\n",
    "    \n",
    "    Parameters:\n",
    "    - D: numpy array of shape (N, d), where N is the number of samples and d is the number of dimensions.\n",
    "    - mu: numpy array of shape (k, d), where k is the number of Gaussians, each mean is a numpy array of shape (d,).\n",
    "    - sigma: numpy array of shape (k, d, d), where each covariance matrix is a numpy array of shape (d, d).\n",
    "    - pi: list of weights of length k, each weight is a float.\n",
    "    \n",
    "    Returns:\n",
    "    - log_likelihoods: numpy array of shape (N,), where each entry is the log-likelihood of the corresponding sample.\n",
    "    \"\"\"\n",
    "    N, d = D.shape\n",
    "    k = mu.shape[0]\n",
    "    \n",
    "    log_likelihoods = np.zeros(N)\n",
    "    \n",
    "    for i in range(N):\n",
    "        total_prob = 0\n",
    "        for j in range(k):\n",
    "            # Use the provided multivariate_gaussian function\n",
    "            prob_density = multivariate_gaussian(D[i], mu[j], sigma[j])\n",
    "            weighted_prob = pi[j] * prob_density\n",
    "            total_prob += weighted_prob\n",
    "        \n",
    "        # Log-likelihood for the i-th data point\n",
    "        log_likelihoods[i] = np.log(total_prob)\n",
    "    \n",
    "    return log_likelihoods\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.patches import Ellipse\n",
    "def draw_ellipse(position, covariance, ax=None, **kwargs):\n",
    "    \"\"\"Draw an ellipse with a given position and covariance\"\"\"\n",
    "    ax = ax or plt.gca()\n",
    "    \n",
    "    # Convert covariance to principal axes\n",
    "    if covariance.shape == (2, 2):\n",
    "        U, s, Vt = np.linalg.svd(covariance)\n",
    "        angle = np.degrees(np.arctan2(U[1, 0], U[0, 0]))\n",
    "        width, height = 2 * np.sqrt(s)\n",
    "    else:\n",
    "        angle = 0\n",
    "        width, height = 2 * np.sqrt(covariance)\n",
    "    \n",
    "    # Draw the ellipse\n",
    "    for nsig in range(1, 2): # We're drawing only 1-sigma (about 68% confidence interval)\n",
    "        ax.add_patch(Ellipse(position, nsig * width, nsig * height, angle, **kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "from ipywidgets import interact, interactive, IntSlider, FloatSlider, Button, VBox, HBox, Output\n",
    "from scipy.stats import norm\n",
    "\n",
    "np.random.seed(12)\n",
    "\n",
    "\n",
    "def plot_gmm(num_gaussians):\n",
    "    # Create synthetic data with three Gaussian clusters\n",
    "    data, cluster_labels = make_blobs(n_samples=100, centers=3, cluster_std=0.60, random_state=0)\n",
    "\n",
    "    # Add some outliers\n",
    "    outliers = np.array([[2, 5], [0, 2], [-2, -2]])\n",
    "    data_with_outliers = np.vstack([outliers, data])\n",
    "    \n",
    "    np.random.seed(12)\n",
    "    clear_output(wait=True)\n",
    "    display(HBox([slider, button]))\n",
    "\n",
    "\n",
    "    # Fit Gaussian Mixture Model\n",
    "    # gmm = GaussianMixture(n_components=num_gaussians, random_state=0)\n",
    "    # gmm.fit(data)\n",
    "\n",
    "    mu, sigma, pi = em_algorithm(data, num_gaussians=num_gaussians, num_iterations=200)\n",
    "\n",
    "\n",
    "\n",
    "    # Calculate likelihoods for data points\n",
    "    inliners = data[np.random.choice(data.shape[0], 3, replace=False)]\n",
    "    samples = np.vstack([outliers, inliners])\n",
    "\n",
    "    log_likelihoods = score_samples(samples, mu, sigma, pi)\n",
    "    likelihoods = np.exp(log_likelihoods)\n",
    "\n",
    "\n",
    "    # Print the likelihoods\n",
    "    for i, likelihood in enumerate(likelihoods):\n",
    "        print(f\"Likelihood for point {chr(65+i)}: {likelihood:.4f}\", end=\"\\t\")\n",
    "        print(f\"Outlier score: {-log_likelihoods[i]:.4f}\")\n",
    "\n",
    "    # Plot the data and the outliers\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    gs = fig.add_gridspec(2, 2,  width_ratios=(7, 2), height_ratios=(2, 7))\n",
    "    \n",
    "    \n",
    "    ax = fig.add_subplot(gs[1, 0])\n",
    "    ax.set_title('Generative Model (Gaussian Mixture)')\n",
    "    ax_x_dist = fig.add_subplot(gs[0, 0], sharex=ax)\n",
    "    ax_y_dist = fig.add_subplot(gs[1, 1], sharey=ax)\n",
    "    \n",
    "    ax.scatter(data_with_outliers[:, 0], data_with_outliers[:, 1], c='blue', alpha=0.15, s=20)\n",
    "    ax.scatter(outliers[:, 0], outliers[:, 1], c='red', marker='x', s=100)  # highlighting outliers\n",
    "    ax.scatter(inliners[:, 0], inliners[:, 1], c='blue', marker='x', s=100)  # highlighting inliniers\n",
    "    for mean, cov in zip(mu, sigma):\n",
    "        draw_ellipse(mean, cov, ax=ax, alpha=0.2, color='green')\n",
    "\n",
    "\n",
    "    # Label the outliers and the chosen inliers\n",
    "    for i, coords in enumerate(samples):\n",
    "        ax.annotate(chr(65+i), (coords[0]+0.3, coords[1]+0.3), fontsize=12, ha='right')\n",
    "        \n",
    "         \n",
    "    # Marginal distributions for each Gaussian\n",
    "    x_range = np.linspace(ax.get_xlim()[0], ax.get_xlim()[1], 100)\n",
    "    y_range = np.linspace(ax.get_ylim()[0], ax.get_ylim()[1], 100)\n",
    "    \n",
    "    for i in range(num_gaussians):\n",
    "        ax_x_dist.plot(x_range, norm.pdf(x_range, mu[i, 0], np.sqrt(sigma[i][0, 0])), color=\"green\", alpha=0.4)\n",
    "        ax_y_dist.plot(norm.pdf(y_range, mu[i, 1], np.sqrt(sigma[i][1, 1])), y_range, color=\"green\", alpha=0.4)\n",
    "\n",
    "    # Hide axis labels for the marginals\n",
    "    ax_x_dist.yaxis.set_ticklabels([])\n",
    "    ax_y_dist.xaxis.set_ticklabels([])\n",
    "    ax_x_dist.yaxis.set_ticks([])\n",
    "    ax_y_dist.xaxis.set_ticks([])\n",
    "    ax_x_dist.axis('off')\n",
    "    ax_y_dist.axis('off')\n",
    "\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "slider = widgets.IntSlider(value=3, min=1, max=20, description='Gaussians:', continuous_update=False)\n",
    "button = widgets.Button(description=\"Plot GMM\")\n",
    "\n",
    "def on_button_clicked(_):\n",
    "    plot_gmm(slider.value)\n",
    "\n",
    "button.on_click(on_button_clicked)\n",
    "display(HBox([slider, button]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
